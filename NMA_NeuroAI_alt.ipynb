{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFyfJ27WTq38"
   },
   "source": [
    "It has been observed in prior work that tasks sharing some underlying structure will exhibit overlapping neural activity. For instance, Yang et. al. (2019) trained RNNs on several cognitive tasks and observed clustering in the neural activity space: some clusters specialized to particular tasks, while others were shared between tasks. In theory, it is also possible to observe a completely distributed representation (i.e. no modular clusters). While Yang focused on sensory tasks, we aim to study tasks involving abstract relations: transitive inference and divisibility. These tasks are likely to have some common underlying structure, as both represent transitive relations. We will compare the neural geometry of the same RNN trained on one of these tasks at a time to that trained on both (using interleaving).\n",
    "\n",
    "Questions we hope to answer: How will the neural representation of a given task change when more than one task is learned simultaneously? In the latter case, will we find that the activations shared between the two tasks are also present in some form when only one task is learned at a time? That isâ€”does a neural network organize its activity differently when related tasks must be learned together? We will use RDM analysis and dimensionality reduction techniques to look for specialized clusters in neural activity space. We will then compare our networks using RSA/RDA, as well as dynamics-based methods such as DSA and fixed/slow point analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "QAxgYljN5Txw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111fd7b30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#from scipy.stats import zscore\n",
    "import random\n",
    "#from statistics import mean\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import GRUCell, RNNCell\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# # Response visualizations\n",
    "# !pip install umap-learn\n",
    "# import umap\n",
    "#import matplotlib as mpl\n",
    "#from matplotlib import pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(12)\n",
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tasks Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz3tYw747mMA"
   },
   "outputs": [],
   "source": [
    "# Transitive Inference: (i.e. index 0 is greater than index 1 = \"A > B\")\n",
    "# Example stimulus: [[1, 0, 0],[0, 1, 0]]\n",
    "# Example output: 0\n",
    "\n",
    "# Subset Inclusion: (i.e. index 0 is greater than index 1 = \"A \\contains B\")\n",
    "# Example stimulus: [[1, 1, 1],[0, 1, 0]]\n",
    "# Example output: 0 = \"A \\contains B\"\n",
    "\n",
    "# Divisibility:\n",
    "# Example stimulus: [6,3]\n",
    "# Example output: 0 = \"A is divisble by B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72VbeOoYExHC",
    "outputId": "6072a5d3-b2c6-4696-d213-fd476d400bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,0,0] < [1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Task Design and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "93onUzVT5a-f"
   },
   "outputs": [],
   "source": [
    "# @title Task design\n",
    "def int2bits(n, width):\n",
    "    '''\n",
    "    Convert an Integer to Its Binary Representation as a List/Array/Tensor\n",
    "    e.g. n = 3, width = 4 -> [0, 0, 1, 1]\n",
    "    '''\n",
    "    return [int(b) for b in bin(n)[2:].zfill(width)] # bin() returns a str with prefix '0b'\n",
    "    \n",
    "\n",
    "\n",
    "def gen_batch(batch_size, n_elements = 3, \n",
    "              task = 'ti'):\n",
    "    '''\n",
    "    Generate a Batch of Stimuli (list) for the Desired Task\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size   : (int) num of stimulus\n",
    "    - n_elements   : (int) Total number of elements on the base set\n",
    "    - task         : (str)\n",
    "                     'ti' - Transitive Inference; without Reflexitivity (relation with itself)\n",
    "                     'si' - Subset Inclusion;\n",
    "                     'div'- Divisibility; without 0\n",
    "\n",
    "    Returns: [[A, B], label] x batch_size       ## of shape [batch_size, 2, n_elements]\n",
    "    '''\n",
    "    assert task in ['ti',\n",
    "                    'si',\n",
    "                    'div'], f'Requested Task [{task}] Not Supported!'\n",
    "    stimuli = []\n",
    "    if 'ti' == task.lower() or 'div' == task.lower():\n",
    "        max_b_size = int(n_elements)\n",
    "        # Transitive Inference\n",
    "        if 'ti' == task.lower():\n",
    "            # All possible instances\n",
    "            stimulus_dict = np.eye(n_elements, dtype = int)\n",
    "            np.random.shuffle(stimulus_dict)\n",
    "            # Randomly sample two instances in the dict as a stimulus\n",
    "            for pair_id in range(batch_size):\n",
    "                dict_indices = random.sample(range(max_b_size), k = 2) # without replacement\n",
    "                # target = which instance is greater\n",
    "                target = 0 if dict_indices[0] > dict_indices[1] else 1\n",
    "                stimuli.append([stimulus_dict[dict_indices].tolist(), target])\n",
    "        else: # Divisibility\n",
    "            for pair_id in range(batch_size):\n",
    "                stimulus_dict = random.sample(range(1, max_b_size + 1), k = 2) # excluding 0\n",
    "                # target = if the previous is divisible by the latter\n",
    "                target = int(0 == stimulus_dict[0] % stimulus_dict[1])\n",
    "                stimuli.append([stimulus_dict, target])\n",
    "    # Subset Inclusion\n",
    "    elif 'si' == task.lower():\n",
    "        max_b_size = 2 ** int(n_elements)\n",
    "        # randomly sample two instances\n",
    "        for pair_id in range(batch_size):\n",
    "            idx_A, idx_B = random.sample(range(max_b_size), k = 2)\n",
    "            # target if the previous is a superset of the latter\n",
    "            A, B = np.array(int2bits(idx_A, width = n_elements)), np.array(int2bits(idx_B, n_elements))\n",
    "            element_indices_B = np.arange(n_elements)[1 == B]\n",
    "            target = int(A[element_indices_B].all()) # if non-zero entries in B are also in A\n",
    "            stimuli.append([[A.tolist(), B.tolist()], target])\n",
    "    return stimuli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[0, 1, 0, 0], [0, 0, 0, 1]], 0],\n",
       " [[[0, 1, 1, 0], [1, 0, 1, 0]], 0],\n",
       " [[[0, 1, 0, 1], [1, 0, 0, 0]], 0],\n",
       " [[[0, 1, 0, 1], [1, 0, 1, 0]], 0],\n",
       " [[[0, 0, 1, 0], [1, 1, 0, 1]], 0],\n",
       " [[[1, 0, 1, 1], [1, 0, 0, 1]], 1],\n",
       " [[[0, 0, 1, 0], [1, 1, 1, 1]], 0],\n",
       " [[[1, 1, 0, 1], [0, 1, 1, 1]], 0],\n",
       " [[[0, 1, 0, 0], [1, 0, 1, 0]], 0],\n",
       " [[[0, 0, 0, 0], [1, 1, 0, 0]], 0]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdata = gen_batch(10, 4, task = 'si')\n",
    "vdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpJyPaV3Fu3E",
    "outputId": "704bcf5a-a4cb-4a43-8c5a-8acae6b6424e"
   },
   "source": [
    "### Data Preparation (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, \n",
    "                 dtype = torch.float, device = 'cpu'):\n",
    "    '''\n",
    "    data : [[A, B], label] x batch_size\n",
    "\n",
    "    returns three tensors of shapes [batch_size, n_elements], [batch_size, n_elements], [n_labels]\n",
    "    '''\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y_t = []\n",
    "    for [A, B], label in data:\n",
    "        x1.append(A)\n",
    "        x2.append(B)\n",
    "        y_t.append(label)\n",
    "    return  torch.tensor(x1, dtype = dtype, device = device), torch.tensor(x2, dtype = dtype, device = device), torch.tensor(y_t, dtype = dtype, device = device)\n",
    "\n",
    "class AbstractTaskDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.raw_data = data\n",
    "        self.data, self.labels = torch.tensor([d for d,_ in data]), torch.tensor([l for _,l in data])\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "### Data Loader, Train_Test split and Shuffle Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_datast = AbstractTaskDataset(vdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_loader = DataLoader(si_datast, batch_size = 2, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 1, 1, 0, 1, 0, 1, 0]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, l = next(iter(t_loader))\n",
    "d.reshape(2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = si_datast[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0],\n",
       "        [0, 0, 0, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x1, x2, y_t = prepare_data(gen_batch(batch_size = 10, n_elements = 5, task = 'si'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim, \n",
    "                 num_layers = 2, dp_rate = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        c_in, c_out = input_dim, hidden_dim\n",
    "        layers = []\n",
    "        for lid in range(num_layers - 1):\n",
    "            layers += [nn.Linear(c_in, c_out),\n",
    "                       nn.LeakyReLU(inplace = True),\n",
    "                       nn.Dropout(dp_rate)]\n",
    "            c_in = hidden_dim\n",
    "        layers += [nn.Linear(c_in, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 2. RNN\n",
    "class GRU_RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim=None, latent_ic_var=0.05\n",
    "    ):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.cell = GRUCell(input_dim, self.hidden_dim)\n",
    "        self.readout = nn.Linear(self.hidden_dim, output_dim, bias=True)\n",
    "        self.latent_ics = torch.nn.Parameter(\n",
    "            torch.zeros(latent_size), requires_grad=True\n",
    "        )\n",
    "        self.latent_ic_var = latent_ic_var\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init_h = self.latent_ics.unsqueeze(0).expand(batch_size, -1)\n",
    "        ic_noise = torch.randn_like(init_h) * self.latent_ic_var\n",
    "        return init_h + ic_noise\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        hidden = self.cell(inputs, hidden)\n",
    "        output = self.readout(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "# 3. Simple RNN\n",
    "class SimpleRNN(nn.Module):\n",
    "    '''\n",
    "    RNN in its Simplest Form using nn.RNNCell\n",
    "    Activation Choices: 'relu' or 'tanh'\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = None, activation = 'relu'):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # Hyperparams\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.cell = RNNCell(input_dim, hidden_dim, nonlinearity = activation)\n",
    "        self.readout = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs, hidden = None):\n",
    "        hidden = self.cell(inputs, hidden)\n",
    "        output = self.readout(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    '''\n",
    "    A Higher Level Model for Three Tasks that Administrates \n",
    "      - Trunnk Part to Generate Common Hidden Representations\n",
    "      - Multiple Downstreaming Readout Heads\n",
    "\n",
    "      ##### Requires Handling of Hidden States for RNN \n",
    "    '''\n",
    "    def __init__(self, trunk_net, hidden_dim, out_dim = 1,\n",
    "                num_layers = 2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.trunk = trunk_net\n",
    "        self.head_mlp = MLP(trunk_net.output_dim, hidden_dim, out_dim, num_layers)\n",
    "        self.head_rnn = SimpleRNN(trunk_net.output_dim, hidden_dim, out_dim) ### TBD\n",
    "\n",
    "    def forward(self, x, hidden = None, task_type = 0):\n",
    "        '''\n",
    "        ### Task Type definition:\n",
    "            - 0 : \n",
    "            - 1 :  other types\n",
    "        '''\n",
    "        x = self.trunk(x)\n",
    "        if 0 != task_type:\n",
    "            x = self.head_mlp(x)\n",
    "            hidden = None\n",
    "        else:\n",
    "            x, hidden = self.head_rnn(x, hidden)\n",
    "        return nn.functional.sigmoid(x), hidden \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example use \n",
    "#trunk_net = MLP(input_dim, hidden_dim, hidden_dim)\n",
    "#mymodel = MyModel(trunk_net, hidden_dim, out_dim = 1)\n",
    "\n",
    "\n",
    "# Example Training (could be wrapped into a helper function)\n",
    "'''\n",
    "# Generic Train Method\n",
    "def train(model, dataloader, tasktype = 0, \n",
    "          loss_func = nn.BCELoss(), optimizer = torch.optim.Adam(lr = 0.01)):\n",
    "    hidden = None\n",
    "    for e in range(n_epochs):\n",
    "        for x, y_t in dataloader:\n",
    "            # Forward Pass\n",
    "            y_pred = mymodel(data, hidden)\n",
    "            loss = loss_func(y_pred, y_t)\n",
    "            # Backward Pass\n",
    "            optimizer.zero_grad() # Clear gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    #return loss or acc lists\n",
    "        \n",
    "\n",
    "# Train with Task Requiring RNN\n",
    "history = train(mymodel, train_data, tasktype = 0)\n",
    "\n",
    "# Train with Task with Linear Readout\n",
    "history = train(mymodel, train_data, tasktype = 1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_test_data(n):\n",
    "    \"\"\"Generate test data for relational tasks.\n",
    "    \"\"\"\n",
    "    test_x = np.zeros((n, n, 2*n))\n",
    "    for i, j in itertools.product(range(n), range(n)):\n",
    "        test_x[i,j,i] = 1 \n",
    "        test_x[i,j,n+j] = 1 \n",
    "    test_x = torch.from_numpy(test_x).float()\n",
    "    test_x = test_x/torch.sqrt(torch.tensor(2))\n",
    "    return test_x\n",
    "\n",
    "def get_transitive_data(n):\n",
    "    \"\"\"Generate training data for TI task.\n",
    "    \"\"\"\n",
    "    test_x = get_test_data(n)\n",
    "    x = test_x[tuple(zip(*([(i, i+1) for i in range(n-1)] + [(i+1, i) for i in range(n-1)])))]\n",
    "    y = torch.tensor([1.]*(n-1)+[-1.]*(n-1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_transitive_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.7071],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
