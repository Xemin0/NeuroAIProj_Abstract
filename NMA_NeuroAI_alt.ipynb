{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFyfJ27WTq38"
   },
   "source": [
    "It has been observed in prior work that tasks sharing some underlying structure will exhibit overlapping neural activity. For instance, Yang et. al. (2019) trained RNNs on several cognitive tasks and observed clustering in the neural activity space: some clusters specialized to particular tasks, while others were shared between tasks. In theory, it is also possible to observe a completely distributed representation (i.e. no modular clusters). While Yang focused on sensory tasks, we aim to study tasks involving abstract relations: transitive inference and divisibility. These tasks are likely to have some common underlying structure, as both represent transitive relations. We will compare the neural geometry of the same RNN trained on one of these tasks at a time to that trained on both (using interleaving).\n",
    "\n",
    "Questions we hope to answer: How will the neural representation of a given task change when more than one task is learned simultaneously? In the latter case, will we find that the activations shared between the two tasks are also present in some form when only one task is learned at a time? That isâ€”does a neural network organize its activity differently when related tasks must be learned together? We will use RDM analysis and dimensionality reduction techniques to look for specialized clusters in neural activity space. We will then compare our networks using RSA/RDA, as well as dynamics-based methods such as DSA and fixed/slow point analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "id": "QAxgYljN5Txw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x123876c70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import zscore\n",
    "import random\n",
    "#from statistics import mean\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "# # Response visualizations\n",
    "# !pip install umap-learn\n",
    "# import umap\n",
    "import matplotlib as mpl\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(12)\n",
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tasks Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz3tYw747mMA"
   },
   "outputs": [],
   "source": [
    "# Transitive Inference: (i.e. index 0 is greater than index 1 = \"A > B\")\n",
    "# Example stimulus: [[1, 0, 0],[0, 1, 0]]\n",
    "# Example output: 0\n",
    "\n",
    "# Subset Inclusion: (i.e. index 0 is greater than index 1 = \"A \\contains B\")\n",
    "# Example stimulus: [[1, 1, 1],[0, 1, 0]]\n",
    "# Example output: 0 = \"A \\contains B\"\n",
    "\n",
    "# Divisibility:\n",
    "# Example stimulus: [6,3]\n",
    "# Example output: 0 = \"A is divisble by B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72VbeOoYExHC",
    "outputId": "6072a5d3-b2c6-4696-d213-fd476d400bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,0,0] < [1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Task Design and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "93onUzVT5a-f"
   },
   "outputs": [],
   "source": [
    "# @title Task design\n",
    "def int2bits(n, width):\n",
    "    '''\n",
    "    Convert an Integer to Its Binary Representation as a List/Array/Tensor\n",
    "    e.g. n = 3, width = 4 -> [0, 0, 1, 1]\n",
    "    '''\n",
    "    return [int(b) for b in bin(n)[2:].zfill(width)] # bin() returns a str with prefix '0b'\n",
    "    \n",
    "\n",
    "\n",
    "def gen_batch(batch_size, n_elements = 3, \n",
    "              task = 'ti'):\n",
    "    '''\n",
    "    Generate a Batch of Stimuli (list) for the Desired Task\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size   : (int) num of stimulus\n",
    "    - n_elements   : (int) Total number of elements on the base set\n",
    "    - task         : (str)\n",
    "                     'ti' - Transitive Inference; without Reflexitivity (relation with itself)\n",
    "                     'si' - Subset Inclusion;\n",
    "                     'div'- Divisibility; without 0\n",
    "\n",
    "    Returns: [[A, B], label] x batch_size       ## of shape [batch_size, 2, n_elements]\n",
    "    '''\n",
    "    assert task in ['ti',\n",
    "                    'si',\n",
    "                    'div'], f'Requested Task [{task}] Not Supported!'\n",
    "    stimuli = []\n",
    "    if 'ti' == task.lower() or 'div' == task.lower():\n",
    "        max_b_size = int(n_elements)\n",
    "        # Transitive Inference\n",
    "        if 'ti' == task.lower():\n",
    "            # All possible instances\n",
    "            stimulus_dict = np.eye(n_elements, dtype = int)\n",
    "            np.random.shuffle(stimulus_dict)\n",
    "            # Randomly sample two instances in the dict as a stimulus\n",
    "            for pair_id in range(batch_size):\n",
    "                dict_indices = random.sample(range(max_b_size), k = 2) # without replacement\n",
    "                # target = which instance is greater\n",
    "                target = 0 if dict_indices[0] > dict_indices[1] else 1\n",
    "                stimuli.append([stimulus_dict[dict_indices].tolist(), target])\n",
    "        else: # Divisibility\n",
    "            for pair_id in range(batch_size):\n",
    "                stimulus_dict = random.sample(range(1, max_b_size + 1), k = 2) # excluding 0\n",
    "                # target = if the previous is divisible by the latter\n",
    "                target = int(0 == stimulus_dict[0] % stimulus_dict[1])\n",
    "                stimuli.append([stimulus_dict, target])\n",
    "    # Subset Inclusion\n",
    "    elif 'si' == task.lower():\n",
    "        max_b_size = 2 ** int(n_elements)\n",
    "        # randomly sample two instances\n",
    "        for pair_id in range(batch_size):\n",
    "            idx_A, idx_B = random.sample(range(max_b_size), k = 2)\n",
    "            # target if the previous is a superset of the latter\n",
    "            A, B = np.array(int2bits(idx_A, width = n_elements)), np.array(int2bits(idx_B, n_elements))\n",
    "            element_indices_B = np.arange(n_elements)[1 == B]\n",
    "            target = int(A[element_indices_B].all()) # if non-zero entries in B are also in A\n",
    "            stimuli.append([[A.tolist(), B.tolist()], target])\n",
    "    return stimuli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[1, 2], 0],\n",
       " [[1, 3], 0],\n",
       " [[1, 6], 0],\n",
       " [[2, 1], 1],\n",
       " [[4, 2], 1],\n",
       " [[6, 3], 1],\n",
       " [[1, 5], 0],\n",
       " [[6, 4], 0],\n",
       " [[2, 5], 0],\n",
       " [[6, 4], 0]]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_batch(10, 5, task = 'div')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpJyPaV3Fu3E",
    "outputId": "704bcf5a-a4cb-4a43-8c5a-8acae6b6424e"
   },
   "source": [
    "### Data Preparation (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, \n",
    "                 dtype = torch.float, device = 'cpu'):\n",
    "    '''\n",
    "    data : [[A, B], label] x batch_size\n",
    "\n",
    "    returns three tensors of shapes [batch_size, n_elements], [batch_size, n_elements], [n_labels]\n",
    "    '''\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y_t = []\n",
    "    for [A, B], label in data:\n",
    "        x1.append(A)\n",
    "        x2.append(B)\n",
    "        y_t.append(label)\n",
    "    return  torch.tensor(x1, dtype = dtype, device = device), torch.tensor(x2, dtype = dtype, device = device), torch.tensor(y_t, dtype = dtype, device = device)\n",
    "\n",
    "\n",
    "### Data Loader, Train_Test split and Shuffle Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x1, x2, y_t = prepare_data(gen_batch(batch_size = 10, n_elements = 5, task = 'si'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim, \n",
    "                 num_layers = 2, dp_rate = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        c_in, c_out = input_dim, hidden_dim\n",
    "        layers = []\n",
    "        for lid in range(num_layers - 1):\n",
    "            layers += [nn.Linear(c_in, c_out),\n",
    "                       nn.LeakyReLU(inplace = True),\n",
    "                       nn.Dropout(dp_rate)]\n",
    "            c_in = hidden_dim\n",
    "        layers += [nn.Linear(c_in, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Model (Classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    '''\n",
    "    A Higher Level Model that takes input\n",
    "    - x1 \n",
    "    - x2 \n",
    "    and output a scalar as the probability for the binary label being 1\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, out_dim = 1,\n",
    "                num_layers = 3):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.feedforward1 = MLP(input_dim, hidden_dim, hidden_dim, num_layers)\n",
    "        self.feedforward2 = MLP(input_dim, hidden_dim, hidden_dim, num_layers)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "            nn.LeakyReLU(inplace = True),\n",
    "            nn.Linear(hidden_dim // 2, out_dim)\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.feedforward1(x1)\n",
    "        x2 = self.feedforward2(x2)\n",
    "        return self.head(x1 + x2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
