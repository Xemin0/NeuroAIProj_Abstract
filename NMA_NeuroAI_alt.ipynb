{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFyfJ27WTq38"
   },
   "source": [
    "It has been observed in prior work that tasks sharing some underlying structure will exhibit overlapping neural activity. For instance, Yang et. al. (2019) trained RNNs on several cognitive tasks and observed clustering in the neural activity space: some clusters specialized to particular tasks, while others were shared between tasks. In theory, it is also possible to observe a completely distributed representation (i.e. no modular clusters). While Yang focused on sensory tasks, we aim to study tasks involving abstract relations: transitive inference and divisibility. These tasks are likely to have some common underlying structure, as both represent transitive relations. We will compare the neural geometry of the same RNN trained on one of these tasks at a time to that trained on both (using interleaving).\n",
    "\n",
    "Questions we hope to answer: How will the neural representation of a given task change when more than one task is learned simultaneously? In the latter case, will we find that the activations shared between the two tasks are also present in some form when only one task is learned at a time? That isâ€”does a neural network organize its activity differently when related tasks must be learned together? We will use RDM analysis and dimensionality reduction techniques to look for specialized clusters in neural activity space. We will then compare our networks using RSA/RDA, as well as dynamics-based methods such as DSA and fixed/slow point analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "QAxgYljN5Txw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12cf0d490>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#from scipy.stats import zscore\n",
    "import random\n",
    "#from statistics import mean\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import GRUCell, RNNCell\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# # Response visualizations\n",
    "# !pip install umap-learn\n",
    "# import umap\n",
    "#import matplotlib as mpl\n",
    "#from matplotlib import pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(12)\n",
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tasks Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vz3tYw747mMA"
   },
   "outputs": [],
   "source": [
    "# Transitive Inference: (i.e. index 0 is greater than index 1 = \"A > B\")\n",
    "# Example stimulus: [[1, 0, 0],[0, 1, 0]]\n",
    "# Example output: 0\n",
    "\n",
    "# Subset Inclusion: (i.e. index 0 is greater than index 1 = \"A \\contains B\")\n",
    "# Example stimulus: [[1, 1, 1],[0, 1, 0]]\n",
    "# Example output: 0 = \"A \\contains B\"\n",
    "\n",
    "# Divisibility:\n",
    "# Example stimulus: [6,3]\n",
    "# Example output: 0 = \"A is divisble by B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72VbeOoYExHC",
    "outputId": "6072a5d3-b2c6-4696-d213-fd476d400bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,0,0] < [1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Task Design and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "93onUzVT5a-f"
   },
   "outputs": [],
   "source": [
    "# @title Task design\n",
    "def int2bits(n, width):\n",
    "    '''\n",
    "    Convert an Integer to Its Binary Representation as a List/Array/Tensor\n",
    "    e.g. n = 3, width = 4 -> [0, 0, 1, 1]\n",
    "    '''\n",
    "    return [int(b) for b in bin(n)[2:].zfill(width)] # bin() returns a str with prefix '0b'\n",
    "    \n",
    "\n",
    "\n",
    "def gen_batch(batch_size, n_elements = 3, \n",
    "              task = 'ti'):\n",
    "    '''\n",
    "    Generate a Batch of Stimuli (list) for the Desired Task\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size   : (int) num of stimulus\n",
    "    - n_elements   : (int) Total number of elements on the base set\n",
    "    - task         : (str)\n",
    "                     'ti' - Transitive Inference; without Reflexitivity (relation with itself)\n",
    "                     'si' - Subset Inclusion; \n",
    "                     'div'- Divisibility; without 0\n",
    "\n",
    "    Returns: [[A, B], label] x batch_size       ## of shape [batch_size, 2, n_elements]\n",
    "    '''\n",
    "    assert task in ['ti',\n",
    "                    'si',\n",
    "                    'div'], f'Requested Task [{task}] Not Supported!'\n",
    "    stimuli = []\n",
    "    if 'ti' == task.lower() or 'div' == task.lower():\n",
    "        max_b_size = int(n_elements)\n",
    "        # Transitive Inference\n",
    "        if 'ti' == task.lower():\n",
    "            # All possible instances\n",
    "            stimulus_dict = np.eye(n_elements, dtype = int)\n",
    "            np.random.shuffle(stimulus_dict)\n",
    "            # Randomly sample two instances in the dict as a stimulus\n",
    "            for pair_id in range(batch_size):\n",
    "                dict_indices = random.sample(range(max_b_size), k = 2) # without replacement\n",
    "                # target = which instance is greater\n",
    "                target = 0 if dict_indices[0] > dict_indices[1] else 1\n",
    "                stimuli.append([stimulus_dict[dict_indices].tolist(), target])\n",
    "        else: # Divisibility\n",
    "            for pair_id in range(batch_size):\n",
    "                stimulus_dict = random.sample(range(1, max_b_size + 1), k = 2) # excluding 0\n",
    "                # target = if the previous is divisible by the latter\n",
    "                target = int(0 == stimulus_dict[0] % stimulus_dict[1])\n",
    "                stimuli.append([stimulus_dict, target])\n",
    "    # Subset Inclusion\n",
    "    elif 'si' == task.lower():\n",
    "        max_b_size = 2 ** int(n_elements)\n",
    "        # randomly sample two instances\n",
    "        for pair_id in range(batch_size):\n",
    "            idx_A, idx_B = random.sample(range(max_b_size), k = 2)\n",
    "            # target if the previous is a superset of the latter\n",
    "            A, B = np.array(int2bits(idx_A, width = n_elements)), np.array(int2bits(idx_B, n_elements))\n",
    "            element_indices_B = np.arange(n_elements)[1 == B]\n",
    "            target = int(A[element_indices_B].all()) # if non-zero entries in B are also in A\n",
    "            stimuli.append([[A.tolist(), B.tolist()], target])\n",
    "    return stimuli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[[0, 1, 0, 0], [1, 1, 0, 1]], 0],\n",
       " [[[0, 0, 1, 1], [0, 0, 0, 0]], 1],\n",
       " [[[0, 0, 0, 0], [1, 1, 0, 0]], 0],\n",
       " [[[1, 1, 0, 0], [0, 0, 1, 0]], 0],\n",
       " [[[1, 0, 1, 0], [0, 1, 1, 1]], 0],\n",
       " [[[1, 1, 1, 1], [0, 1, 1, 0]], 1],\n",
       " [[[0, 1, 1, 0], [0, 1, 1, 1]], 0],\n",
       " [[[1, 1, 1, 0], [0, 0, 1, 1]], 0],\n",
       " [[[1, 0, 1, 1], [0, 1, 1, 1]], 0],\n",
       " [[[1, 1, 1, 1], [1, 0, 0, 1]], 1]]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vdata = gen_batch(10, 4, task = 'si')\n",
    "vdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 2, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZpJyPaV3Fu3E",
    "outputId": "704bcf5a-a4cb-4a43-8c5a-8acae6b6424e"
   },
   "source": [
    "### Data Preparation (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, \n",
    "                 dtype = torch.float, device = 'cpu'):\n",
    "    '''\n",
    "    data : [[A, B], label] x batch_size\n",
    "\n",
    "    returns three tensors of shapes [batch_size, n_elements], [batch_size, n_elements], [n_labels]\n",
    "    '''\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    y_t = []\n",
    "    for [A, B], label in data:\n",
    "        x1.append(A)\n",
    "        x2.append(B)\n",
    "        y_t.append(label)\n",
    "    return  torch.tensor(x1, dtype = dtype, device = device), torch.tensor(x2, dtype = dtype, device = device), torch.tensor(y_t, dtype = dtype, device = device)\n",
    "\n",
    "class AbstractTaskDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.raw_data = data\n",
    "        #self.data, self.labels = torch.tensor([d for d,_ in data]), torch.tensor([l for _,l in data])\n",
    "        self.data, self.labels = [d for d,_ in data],[l for _,l in data]\n",
    "    def __getitem__(self, idx):\n",
    "        #return torch.tensor(self.raw_data[idx][0]), torch.tensor(self.raw_data[idx][1])\n",
    "        return torch.tensor(self.data[idx], dtype = torch.float), torch.tensor(self.labels[idx], dtype = torch.float)\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "    \n",
    "### Data Loader, Train_Test split and Shuffle Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_datast = AbstractTaskDataset(vdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(si_datast, batch_size = 4, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 1., 0., 0.],\n",
       "         [1., 1., 0., 1.]],\n",
       "\n",
       "        [[0., 0., 1., 1.],\n",
       "         [0., 0., 0., 0.]],\n",
       "\n",
       "        [[0., 0., 0., 0.],\n",
       "         [1., 1., 0., 0.]],\n",
       "\n",
       "        [[1., 1., 0., 0.],\n",
       "         [0., 0., 1., 0.]]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d, l = next(iter(train_loader))\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = si_datast[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "x1, x2, y_t = prepare_data(gen_batch(batch_size = 10, n_elements = 5, task = 'si'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLP\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, \n",
    "                 num_layers = 2, dp_rate = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        c_in, c_out = input_dim, hidden_dim\n",
    "        layers = []\n",
    "        for lid in range(num_layers - 1):\n",
    "            layers += [nn.Linear(c_in, c_out),\n",
    "                       nn.LeakyReLU(inplace = True),\n",
    "                       nn.Dropout(dp_rate)]\n",
    "            c_in = hidden_dim\n",
    "        layers += [nn.Linear(c_in, c_out)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# 2. RNN\n",
    "class GRU_RNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim, hidden_dim, output_dim=None, latent_ic_var=0.05\n",
    "    ):\n",
    "        super(GRU_RNN, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.cell = GRUCell(input_dim, self.hidden_dim)\n",
    "        self.readout = nn.Linear(self.hidden_dim, output_dim, bias=True)\n",
    "        self.latent_ics = torch.nn.Parameter(\n",
    "            torch.zeros(latent_size), requires_grad=True\n",
    "        )\n",
    "        self.latent_ic_var = latent_ic_var\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        init_h = self.latent_ics.unsqueeze(0).expand(batch_size, -1)\n",
    "        ic_noise = torch.randn_like(init_h) * self.latent_ic_var\n",
    "        return init_h + ic_noise\n",
    "\n",
    "    def forward(self, inputs, hidden):\n",
    "        hidden = self.cell(inputs, hidden)\n",
    "        output = self.readout(hidden)\n",
    "        return output, hidden\n",
    "\n",
    "# 3. Simple RNN\n",
    "class SimpleRNN(nn.Module):\n",
    "    '''\n",
    "    RNN in its Simplest Form using nn.RNNCell\n",
    "    Activation Choices: 'relu' or 'tanh'\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = None, activation = 'relu'):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # Hyperparams\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        self.cell = RNNCell(input_dim, hidden_dim, nonlinearity = activation)\n",
    "        self.readout = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, inputs, hidden = None):\n",
    "        hidden = self.cell(inputs, hidden)\n",
    "        output = self.readout(hidden)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    '''\n",
    "    A Higher Level Model for Three Tasks that Administrates \n",
    "      - Trunnk Part to Generate Common Hidden Representations\n",
    "      - Multiple Downstreaming Readout Heads\n",
    "\n",
    "      ##### Requires Handling of Hidden States for RNN \n",
    "    '''\n",
    "    def __init__(self, trunk_net, hidden_dim, output_dim = 1,\n",
    "                num_layers = 2):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.trunk = trunk_net\n",
    "        self.head_mlp = MLP(trunk_net.output_dim, hidden_dim, output_dim, num_layers)\n",
    "        self.head_rnn = SimpleRNN(trunk_net.output_dim, hidden_dim, output_dim) ### TBD\n",
    "\n",
    "    def forward(self, x, hidden = None, task_type = 0):\n",
    "        '''\n",
    "        ### Task Type definition:\n",
    "            - 0 : \n",
    "            - 1 :  other types\n",
    "        '''\n",
    "        x = self.trunk(x)\n",
    "        if 0 != task_type:\n",
    "            x = self.head_mlp(x)\n",
    "            hidden = None\n",
    "        else:\n",
    "            x, hidden = self.head_rnn(x, hidden)\n",
    "        return nn.functional.sigmoid(x).squeeze(), hidden \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Train Method ### Does not handle the devices for now\n",
    "def test(model, dataloader, task_type = 0, loss_func = nn.BCELoss()):\n",
    "    # Eval Mode\n",
    "    model.eval()\n",
    "    hidden = None\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, y_t in dataloader:\n",
    "            b_size = x.shape[0]\n",
    "            y_p = model(x.reshape(b_size, -1), hidden)\n",
    "            loss = loss_func(preds, y_t).clone().detach()\n",
    "            acc = (y_p == y_t).sum().item() / len(y_t)\n",
    "\n",
    "            # Record statistics\n",
    "            total_loss += loss\n",
    "            total_acc += acc\n",
    "    # Back to Train Mode\n",
    "    model.train()\n",
    "    return total_loss.clone().detach() / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader = None, n_epochs = 20, \n",
    "          tasktype = 0, lr = 0.01,\n",
    "          loss_func = nn.BCELoss()):\n",
    "    hidden = None\n",
    "    loss_lst = []\n",
    "    acc_lst = []\n",
    "    test_acc_lst = []\n",
    "    # Using Adam by default\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.train()\n",
    "    for e in range(n_epochs + 1):\n",
    "        for x, y_t in train_dataloader:\n",
    "            # Forward Pass\n",
    "            y_pred, hidden = mymodel(x.reshape(x.shape[0], -1), hidden)\n",
    "            loss = loss_func(y_pred, y_t)\n",
    "            # Backward Pass\n",
    "            optimizer.zero_grad() # Clear gradients\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record Statistics\n",
    "            loss_lst.append(loss.clone().detach())\n",
    "            acc = (y_pred == y_t).sum().item() / len(y_t)\n",
    "            acc_lst.append(acc)\n",
    "        if 0 == (e+1) % 10:\n",
    "            t_loss = 0.0\n",
    "            t_acc = 0.0\n",
    "            if test_dataloader is not None:\n",
    "                t_loss, t_acc = test(model, test_dataloader, task_type, loss_func)\n",
    "            print(f'Epoch [{e}/{n_epochs}]:\\t--LastBatchLoss:{loss:.3f}, TrainAcc:{acc:.3f}, TestLoss:{t_loss:.3f}, TestAcc:{t_acc:.3f}')\n",
    "\n",
    "    return loss_lst, acc_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x shape =  torch.Size([4, 2, 4])\n",
      "nets input dim =  8\n",
      "nets output dim =  16\n",
      "hello\n",
      "x shape =  torch.Size([4, 2, 4])\n",
      "nets input dim =  8\n",
      "nets output dim =  16\n",
      "hello\n",
      "x shape =  torch.Size([2, 2, 4])\n",
      "nets input dim =  8\n",
      "nets output dim =  16\n",
      "hello\n"
     ]
    }
   ],
   "source": [
    "for x, y_t in train_loader:\n",
    "    print('x shape = ', x.shape)\n",
    "    print('nets input dim = ', trunk_net.input_dim)\n",
    "    print('nets output dim = ', trunk_net.output_dim)\n",
    "    b_size = x.shape[0]\n",
    "    y_pred = trunk_net(x.reshape(b_size, -1))\n",
    "    print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[99], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m mymodel \u001b[38;5;241m=\u001b[39m MyModel(trunk_net, hidden_dim, output_dim \u001b[38;5;241m=\u001b[39m output_dim)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Train with Task Requiring RNN\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m l_lst, a_lst \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmymodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtasktype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Train with Task with Linear Readout\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#history = train(mymodel, train_data, tasktype = 1)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[98], line 40\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, n_epochs, tasktype, lr, loss_func)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Backward Pass\u001b[39;00m\n\u001b[1;32m     39\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad() \u001b[38;5;66;03m# Clear gradients\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Record Statistics\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/DL/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/DL/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "### Example use \n",
    "input_dim = 4\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "trunk_net = MLP(input_dim * 2, hidden_dim, hidden_dim)\n",
    "mymodel = MyModel(trunk_net, hidden_dim, output_dim = output_dim)\n",
    "\n",
    "# Train with Task Requiring RNN\n",
    "l_lst, a_lst = train(mymodel, train_loader, tasktype = 0, n_epochs = 20, lr = 0.01)\n",
    "\n",
    "# Train with Task with Linear Readout\n",
    "#history = train(mymodel, train_data, tasktype = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Task design\n",
    "def int2bits(n, width):\n",
    "    '''\n",
    "    Convert an Integer to Its Binary Representation as a List/Array/Tensor\n",
    "    e.g. n = 3, width = 4 -> [0, 0, 1, 1]\n",
    "    '''\n",
    "    return [int(b) for b in bin(n)[2:].zfill(width)] # bin() returns a str with prefix '0b'\n",
    "    \n",
    "\n",
    "\n",
    "def gen_batch(batch_size, n_elements = 3, \n",
    "              task = 'ti'):\n",
    "    '''\n",
    "    Generate a Batch of Stimuli (list) for the Desired Task\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size   : (int) num of stimulus\n",
    "    - n_elements   : (int) Total number of elements on the base set\n",
    "    - task         : (str)\n",
    "                     'ti' - Transitive Inference; without Reflexitivity (relation with itself)\n",
    "                     'si' - Subset Inclusion; \n",
    "                     'div'- Divisibility; without 0\n",
    "\n",
    "    Returns: [[A, B], label] x batch_size       ## of shape [batch_size, 2, n_elements]\n",
    "    '''\n",
    "    assert task in ['ti',\n",
    "                    'si',\n",
    "                    'div'], f'Requested Task [{task}] Not Supported!'\n",
    "    stimuli = []\n",
    "    if 'ti' == task.lower() or 'div' == task.lower():\n",
    "        max_b_size = int(n_elements)\n",
    "        # Transitive Inference\n",
    "        if 'ti' == task.lower():\n",
    "            # All possible instances\n",
    "            stimulus_dict = np.eye(n_elements, dtype = int)\n",
    "            np.random.shuffle(stimulus_dict)\n",
    "            # Randomly sample two instances in the dict as a stimulus\n",
    "            for pair_id in range(batch_size):\n",
    "                dict_indices = random.sample(range(max_b_size), k = 2) # without replacement\n",
    "                # target = which instance is greater\n",
    "                target = 0 if dict_indices[0] > dict_indices[1] else 1\n",
    "                stimuli.append([stimulus_dict[dict_indices].tolist(), target])\n",
    "        else: # Divisibility\n",
    "            for pair_id in range(batch_size):\n",
    "                stimulus_dict = random.sample(range(1, max_b_size + 1), k = 2) # excluding 0\n",
    "                # target = if the previous is divisible by the latter\n",
    "                target = int(0 == stimulus_dict[0] % stimulus_dict[1])\n",
    "                stimuli.append([stimulus_dict, target])\n",
    "    # Subset Inclusion\n",
    "    elif 'si' == task.lower():\n",
    "        max_b_size = 2 ** int(n_elements)\n",
    "        # randomly sample two instances\n",
    "        for pair_id in range(batch_size):\n",
    "            idx_A, idx_B = random.sample(range(max_b_size), k = 2)\n",
    "            # target if the previous is a superset of the latter\n",
    "            A, B = np.array(int2bits(idx_A, width = n_elements)), np.array(int2bits(idx_B, n_elements))\n",
    "            element_indices_B = np.arange(n_elements)[1 == B]\n",
    "            target = int(A[element_indices_B].all()) # if non-zero entries in B are also in A\n",
    "            stimuli.append([[A.tolist(), B.tolist()], target])\n",
    "    return stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_test_data(n):\n",
    "    \"\"\"Generate test data for relational tasks.\n",
    "    \"\"\"\n",
    "    test_x = np.zeros((n, n, 2*n))\n",
    "    for i, j in itertools.product(range(n), range(n)):\n",
    "        test_x[i,j,i] = 1 \n",
    "        test_x[i,j,n+j] = 1 \n",
    "    test_x = torch.from_numpy(test_x).float()\n",
    "    test_x = test_x/torch.sqrt(torch.tensor(2))\n",
    "    return test_x\n",
    "\n",
    "def get_transitive_data(n):\n",
    "    \"\"\"Generate training data for TI task.\n",
    "    \"\"\"\n",
    "    test_x = get_test_data(n)\n",
    "    x = test_x[tuple(zip(*([(i, i+1) for i in range(n-1)] + [(i+1, i) for i in range(n-1)])))]\n",
    "    y = torch.tensor([1.]*(n-1)+[-1.]*(n-1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_transitive_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.7071],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
