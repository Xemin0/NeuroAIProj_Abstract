{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fFyfJ27WTq38"
   },
   "source": [
    "It has been observed in prior work that tasks sharing some underlying structure will exhibit overlapping neural activity. For instance, Yang et. al. (2019) trained RNNs on several cognitive tasks and observed clustering in the neural activity space: some clusters specialized to particular tasks, while others were shared between tasks. In theory, it is also possible to observe a completely distributed representation (i.e. no modular clusters). While Yang focused on sensory tasks, we aim to study tasks involving abstract relations: transitive inference and divisibility. These tasks are likely to have some common underlying structure, as both represent transitive relations. We will compare the neural geometry of the same RNN trained on one of these tasks at a time to that trained on both (using interleaving).\n",
    "\n",
    "Questions we hope to answer: How will the neural representation of a given task change when more than one task is learned simultaneously? In the latter case, will we find that the activations shared between the two tasks are also present in some form when only one task is learned at a time? That isâ€”does a neural network organize its activity differently when related tasks must be learned together? We will use RDM analysis and dimensionality reduction techniques to look for specialized clusters in neural activity space. We will then compare our networks using RSA/RDA, as well as dynamics-based methods such as DSA and fixed/slow point analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "cellView": "form",
    "id": "QAxgYljN5Txw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12cf2daf0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# General\n",
    "import numpy as np\n",
    "#import pandas as pd\n",
    "#from scipy.stats import zscore\n",
    "import random\n",
    "#from statistics import mean\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# # Response visualizations\n",
    "# !pip install umap-learn\n",
    "# import umap\n",
    "#import matplotlib as mpl\n",
    "#from matplotlib import pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(12)\n",
    "torch.manual_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Tasks Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Vz3tYw747mMA"
   },
   "outputs": [],
   "source": [
    "# Transitive Inference: (i.e. index 0 is greater than index 1 = \"A > B\")\n",
    "# Example stimulus: [[1, 0, 0],[0, 1, 0]]\n",
    "# Example output: 0\n",
    "\n",
    "# Subset Inclusion: (i.e. index 0 is greater than index 1 = \"A \\contains B\")\n",
    "# Example stimulus: [[1, 1, 1],[0, 1, 0]]\n",
    "# Example output: 0 = \"A \\contains B\"\n",
    "\n",
    "# Divisibility:\n",
    "# Example stimulus: [6,3]\n",
    "# Example output: 0 = \"A is divisble by B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "72VbeOoYExHC",
    "outputId": "6072a5d3-b2c6-4696-d213-fd476d400bbc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,0,0] < [1,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### RNN Task Design and Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "93onUzVT5a-f"
   },
   "outputs": [],
   "source": [
    "# @title Task design\n",
    "def int2bits(n, width):\n",
    "    '''\n",
    "    Convert an Integer to Its Binary Representation as a List/Array/Tensor\n",
    "    e.g. n = 3, width = 4 -> [0, 0, 1, 1]\n",
    "    '''\n",
    "    return [int(b) for b in bin(n)[2:].zfill(width)] # bin() returns a str with prefix '0b'\n",
    "    \n",
    "\n",
    "def make_stimulus_dict(input_length, task='ti'):\n",
    "    stimulus_order_dict = []\n",
    "\n",
    "    if task == 'ti':\n",
    "      for index in range(input_length):\n",
    "        stimulus_key = [0]*input_length\n",
    "        stimulus_key[index] = 1\n",
    "        stimulus_order_dict.append(stimulus_key)\n",
    "      random.shuffle(stimulus_order_dict)\n",
    "\n",
    "    # elif task == \"si\":\n",
    "    #   for index in range(input_length):\n",
    "    #     stimulus_key = [0]*input_length\n",
    "\n",
    "\n",
    "    return stimulus_order_dict\n",
    "\n",
    "def generate_RNN_trial(trial_length, item_dictionary, item_length, output_size):\n",
    "  items = item_dictionary.copy()\n",
    "  sorting_func = lambda x : item_dictionary.index(x)\n",
    "  random.shuffle(items)\n",
    "  inputs = []\n",
    "  inputs_flat = []\n",
    "  outputs = []\n",
    "  for i in range(trial_length):\n",
    "    two_samples = [0,0]\n",
    "    while two_samples[0] == two_samples[1]:\n",
    "      two_samples = random.sample(items,2)\n",
    "    inputs.extend([two_samples])\n",
    "    inputs_flat.extend(two_samples)\n",
    "    inputs_sorted = [sorted(inputs_flat, key=sorting_func)]\n",
    "    for i in inputs_sorted:\n",
    "      current_size = len(i)\n",
    "      dummy_vec = [0]*item_length\n",
    "      i.extend([dummy_vec]*(output_size-current_size))\n",
    "    outputs.extend(inputs_sorted)\n",
    "\n",
    "  inputs = torch.tensor(inputs).float()\n",
    "  outputs = torch.tensor(outputs).float()\n",
    "\n",
    "  return inputs, outputs\n",
    "\n",
    "def get_RNN_batch(num_trials, trial_length, item_dictionary, item_length, pad=False):\n",
    "  if pad:\n",
    "    output_size = trial_length * 2\n",
    "  else: output_size = 0\n",
    "\n",
    "  trials = []\n",
    "  for n in range(num_trials):\n",
    "    trials.append(generate_RNN_trial(trial_length, item_dictionary, item_length, output_size))\n",
    "  return trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size, task, stimulus_dict=None, input_length=3, mode='train'):\n",
    "\n",
    "  # Helpers\n",
    "  def get_binary_vec(input_length):\n",
    "    return [random.randint(0, 1) for _ in range(input_length)]\n",
    "\n",
    "  def make_stimulus(input_length, task='ti'):\n",
    "    if task == 'ti':\n",
    "      if mode == 'test':\n",
    "        dict_indices = [0,0]\n",
    "        while abs(dict_indices[0] - dict_indices[1]) < 2:\n",
    "          dict_indices = random.sample(range(input_length), 2)\n",
    "          random.shuffle(dict_indices)\n",
    "      elif mode == 'train':\n",
    "        temp_idx = random.sample(range(input_length-1), 1)\n",
    "        dict_indices = [temp_idx[0], temp_idx[0]+1]\n",
    "        random.shuffle(dict_indices)\n",
    "\n",
    "      stimulus = [stimulus_dict[i] for i in dict_indices]\n",
    "\n",
    "      if dict_indices[0] < dict_indices[1]: # If the first item is greater, it appears earlier in the list\n",
    "        target = 0\n",
    "      else:\n",
    "        target = 1\n",
    "\n",
    "    elif task == 'si':\n",
    "      stimulus = [0,0]\n",
    "      if mode == 'test':\n",
    "        # while stimulus[0] == stimulus[1]:\n",
    "        #   stimulus = [get_binary_vec(input_length), get_binary_vec(input_length)]\n",
    "        first_vec = get_binary_vec(input_length)\n",
    "        while stimulus[0] == stimulus[1]:\n",
    "          temp_indices = random.sample(range(input_length), random.randint(2, input_length))\n",
    "          second_vec = first_vec.copy()\n",
    "          for i in temp_indices:\n",
    "            if second_vec[i] == 1:\n",
    "              second_vec[i] = 0\n",
    "            # else:\n",
    "            #   second_vec[i] = 1\n",
    "          stimulus = [first_vec, second_vec]\n",
    "      elif mode == 'train':\n",
    "        first_vec = get_binary_vec(input_length)\n",
    "        temp_idx = random.sample(range(input_length), 1)\n",
    "        second_vec = first_vec.copy()\n",
    "        while stimulus[0] == stimulus[1]:\n",
    "          if second_vec[temp_idx[0]] == 1:\n",
    "            second_vec[temp_idx[0]] = 0\n",
    "          else:\n",
    "            second_vec[temp_idx[0]] = 1\n",
    "          stimulus = [first_vec, second_vec]\n",
    "\n",
    "      random.shuffle(stimulus)\n",
    "      if stimulus[0] > stimulus[1]: # If first binary number is greater, it is the superset\n",
    "        target = 0\n",
    "      else:\n",
    "        target = 1\n",
    "\n",
    "    # elif task == \"div\":\n",
    "    #   stimulus = [random.sample(range(0, 100, 2),1)[0], random.sample(range(0, 100, 2),1)[0]]\n",
    "    #   if stimulus[0] % stimulus[1] == 0:\n",
    "    #     target = 0\n",
    "    #   else:\n",
    "    #     target = 1\n",
    "\n",
    "    return stimulus, target\n",
    "\n",
    "  stimuli = []\n",
    "  labels = []\n",
    "  for _ in range(batch_size):\n",
    "    stimulus, target = make_stimulus(input_length, task)\n",
    "    stimuli.append(stimulus)\n",
    "    labels.append(target)\n",
    "\n",
    "  stimuli = torch.tensor(stimuli).float()\n",
    "  labels = torch.tensor(labels).float()\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    stimuli = stimuli.to('cuda')\n",
    "    labels = labels.to('cuda')\n",
    "\n",
    "  return stimuli, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base Backbones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. MLP\n",
    "class MLP(nn.Module):\n",
    "    '''\n",
    "    MLP with Leaky ReLU as Intermediate Activations\n",
    "    input_dim  :\n",
    "    hidden_dim :\n",
    "    output_dim :\n",
    "    num_layers :\n",
    "    dp_rate    : dropout rate\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, \n",
    "                 num_layers = 2, dp_rate = 0.1):\n",
    "        super(MLP, self).__init__()\n",
    "        # Hyperparameters\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        c_in, c_out = input_dim, hidden_dim\n",
    "        layers = []\n",
    "        for lid in range(num_layers - 1):\n",
    "            layers += [nn.Linear(c_in, c_out),\n",
    "                       nn.LeakyReLU(inplace = True),\n",
    "                       nn.Dropout(dp_rate)]\n",
    "            c_in = hidden_dim\n",
    "        layers += [nn.Linear(c_in, output_dim)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "# 2. Simple RNN\n",
    "class SimpleRNN(nn.Module):\n",
    "    '''\n",
    "    RNN in its Simplest Form using nn.RNN\n",
    "    Activation Choices: 'relu' or 'tanh'\n",
    "\n",
    "        - input_dim  : input feat dim\n",
    "        - hidden_dim : \n",
    "        - output_dim : \n",
    "        - num_layers : number of RNN units\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim = None, \n",
    "                 num_layers = 2, activation = 'relu'):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        # Hyperparams\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers,\n",
    "                          batch_first = True, nonlinearity = activation)\n",
    "        self.readout = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        '''\n",
    "        Shape\n",
    "        Inputs:\n",
    "            - x      : [batch_size, seq_len, input_dim]\n",
    "            - h      : [num_layers, batch_size, hidden_dim]\n",
    "        ----\n",
    "        Intermediate:\n",
    "            - out    : [batch_size, seq_len, hidden_dim] # taking the last sequence in the out as the final output pred\n",
    "            - hidden : [num_layers, batch_size, hidden_dim]\n",
    "        ----\n",
    "        Outputs:\n",
    "            - output : [batch_size, output_dim] \n",
    "            - hidden : [num_layers, batch_size, hidden_dim] \n",
    "        '''\n",
    "        out, hidden = self.rnn(x, h)\n",
    "        # Return pred of each seq in the batch\n",
    "        output = self.readout(out[:, -1,:]) \n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrderPredRNN(nn.Module):\n",
    "    '''\n",
    "    Task specific RNN\n",
    "    Activation Choices: 'relu' or 'tanh'\n",
    "\n",
    "        - input_dim  : input feat dim\n",
    "        - hidden_dim : \n",
    "        - n_pairs    : \n",
    "        - num_layers : number of RNN units\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, n_pairs, \n",
    "                 num_layers = 2, activation = 'relu'):\n",
    "        super(OrderPredRNN, self).__init__()\n",
    "        # Hyperparams\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_pairs = n_pairs\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers,\n",
    "                          batch_first = True, nonlinearity = activation)\n",
    "        self.readout = nn.Linear(hidden_dim, 4 * n_pairs * n_pairs)\n",
    "\n",
    "    def forward(self, x, h = None):\n",
    "        '''\n",
    "        Shape\n",
    "        Inputs:\n",
    "            - x      : [batch_size, n_pairs, input_dim]\n",
    "            - h      : [num_layers, batch_size, hidden_dim]\n",
    "        ----\n",
    "        Intermediate:\n",
    "            - out    : [batch_size, n_pairs, hidden_dim] # k-th vector in the n_pair dimension corresponds to information for the first k-pairs of inputs\n",
    "            - hidden : [num_layers, batch_size, hidden_dim]\n",
    "        ----\n",
    "        Outputs:\n",
    "            - output : [batch_size, n_pairs, 2n_pairs, 2*n_pairs ] each matrix in n_pairs dim is a 2*n_pairs x 2*n_pairs matrix indicating the probability of each element to be present at each position \n",
    "            - hidden : [num_layers, batch_size, hidden_dim] Abandoned\n",
    "        '''\n",
    "        out, hidden = self.rnn(x, h)\n",
    "        # Return pred of each seq in the batch\n",
    "        output = self.readout(out)  # everything \n",
    "        \n",
    "        output = output.view(-1, n_pairs, 2*self.n_pairs, 2*self.n_pairs)\n",
    "        output = F.softmax(output, dim = -1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "output, hidden = RNN(input)_torch\n",
    "input (seq_len, feat_dim) : (2, 1)\n",
    "output (seq_len, hidden_dim)\n",
    "\n",
    "hidden: [num_rnn_unit, batch_size, hidden_dim = 10]\n",
    "\n",
    "readout(hidden): [num_rnn_unit, batch_size, output_dim] \n",
    "\n",
    "output(-1, hidden_dim)\n",
    "\n",
    "RNN - Fibbonaci \n",
    "(1, 1) -> [1, 1, (2)]\n",
    "(1, 2) -> [1, 1, 2, (3)]\n",
    "============\n",
    "0: (1, 2) - > [1, 2, 0, 0, 0, 0] \n",
    "1: (3, 2) - > [1, 2, 2, 3, 0, 0]\n",
    "2: (4, 7) - > [1, 2, 2, 2, 3, 7]\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "batch_size = 3\n",
    "expected_output_size = 6\n",
    "readout(hidden): [num_rnn_unit, batch_size = 3, output_dim = 1] \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    '''\n",
    "    A Higher Level Model for Three Tasks that Administrates \n",
    "      - Trunnk Part to Generate Common Hidden Representations\n",
    "      - Multiple Downstreaming Readout Heads\n",
    "\n",
    "      Trunk Net : Produces the relation for each input pair as the context input\n",
    "      RNN       : input(original_inputs, hidden = output of trunk_net)\n",
    "\n",
    "      ##### Trunk Net's output dimension needs to be an even number for RNN to work\n",
    "      ##### Requires Handling of Hidden States for RNN \n",
    "      #####\n",
    "    '''\n",
    "    def __init__(self, trunk_net, hidden_dim, n_pairs, output_dim = 1,\n",
    "                num_layers = 2):\n",
    "        '''\n",
    "        \n",
    "        '''\n",
    "        super(MyModel, self).__init__()\n",
    "        # Parameters\n",
    "        self.hidden_dim = hidden_dim # Even number\n",
    "        self.output_dim = output_dim\n",
    "        self.n_pairs = n_pairs\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.trunk = trunk_net\n",
    "\n",
    "        self.fc1 = nn.Linear(n_pairs * trunk_net.output_dim, trunk_net.output_dim)\n",
    "            \n",
    "        self.head_mlp = MLP(trunk_net.output_dim, hidden_dim, output_dim = output_dim, num_layers = num_layers)\n",
    "        self.head_rnn = OrderPredRNN(trunk_net.input_dim, hidden_dim = trunk_net.output_dim, \n",
    "                                     n_pairs = n_pairs, num_layers = num_layers) ###\n",
    "\n",
    "    def forward(self, x, task_type = 0): # pred = model(x, task_type = 1) to use the FF\n",
    "        '''\n",
    "        ### Task Type definition:\n",
    "            - 0 : \n",
    "            - 1 :  other types FF\n",
    "\n",
    "            - x : [batch_size, n_pairs, 2 * feat_dim = trunk_net.input_dim]\n",
    "\n",
    "            -out: [batch_size, n_pairs, 2*n_pairs, 2*n_pairs] for task 0\n",
    "                : [batch_size, n_pairs, output_dim] for task 1\n",
    "        '''\n",
    "        out_trunk = self.trunk(x) # [batch_size, n_pairs, hidden_dim = trunk_net.output_dim]\n",
    "        batch_size = x.shape[0]\n",
    "        if 0 == task_type:\n",
    "            # Prepare the trunk output as the hidden state/context, [num_layers, batch_size, trunk_net.output_dim]\n",
    "            out_trunk = self.fc1(out_trunk.reshape(batch_size, -1))\n",
    "            hidden = out_trunk.expand((self.num_layers, batch_size, -1))\n",
    "            \n",
    "            out = self.head_rnn(x, hidden) # [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "        else:\n",
    "            out = nn.functional.sigmoid(self.head_mlp(out_trunk)).squeeze()\n",
    "\n",
    "        return out\n",
    "\n",
    "    def get_mask(self, batch_size):\n",
    "        '''\n",
    "        mask of shape [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "        for the output\n",
    "        '''\n",
    "        mask = torch.zeros((self.n_pairs, 2*self.n_pairs, 2*self.n_pairs), dtype = torch.bool)\n",
    "        for k in range(self.n_pairs):\n",
    "            mask[k, :2*(k+1), :2*(k+1)] = 1\n",
    "        return mask.expand((batch_size, self.n_pairs, 2*self.n_pairs, 2*self.n_pairs))\n",
    "        \n",
    "    def infer_orders(self, x, one_hot = True):\n",
    "        '''\n",
    "        x       : [batch_size, n_pairs, 2 * feat_dim = trunk_net.input_dim]\n",
    "        returns : [batch_size, n_pairs, 2*n_pairs, feat_dim]\n",
    "        '''\n",
    "        batch_size = x.shape[0]\n",
    "        feat_dim = self.trunk.input_dim // 2\n",
    "        orders = torch.zeros((batch_size, self.n_pairs, 2*self.n_pairs, feat_dim))\n",
    "\n",
    "        # Creating a copy of all the input accessed by index\n",
    "        element_list = x.clone().reshape((batch_size, 2 * self.n_pairs, -1))\n",
    "        \n",
    "        logits = self.forward(x) # [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "        mask = self.get_mask(batch_size)\n",
    "\n",
    "        masked_logits = logits * mask\n",
    "        # for each sequence(0 ~ n_pairs) of length 2*n_pairs\n",
    "        orders_with_indices = masked_logits.argmax(axis = -1) # [batch_size, n_pairs, 2*n_pairs]\n",
    "        \n",
    "        for b in range(batch_size):\n",
    "            for k in range(self.n_pairs):\n",
    "                for i in range(2*(k+1)):\n",
    "                    orders[b, k, i] = element_list[b, orders_with_indices[b, k, i]]\n",
    "        return orders\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom loss for RNN task\n",
    "def loss_RNNtask(y_p, y_t):\n",
    "    '''\n",
    "    y_p : [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "    y_t : [batch_size, n_pairs, 2*n_pairs]\n",
    "    \n",
    "    ### May need to adjust the calculation since the masked out entries are still used for loss ###\n",
    "    '''\n",
    "    # Reshape to fit nn.CrossEntropyLoss()\n",
    "    batch_size, n_pairs = y_t.shape[:2]\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    y_p = y_p.view((batch_size*n_pairs, 2*n_pairs, 2*n_pairs))\n",
    "    y_t = y_t.view((batch_size*n_pairs, -1))\n",
    "    return criterion(y_p, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic Train Method ### Does not handle the devices for now\n",
    "def test(model, dataloader, task_type = 0):\n",
    "    '''\n",
    "    task_type : 0 for RNN; 1 for FF\n",
    "    '''\n",
    "    # Eval Mode\n",
    "    model.eval()\n",
    "    total_acc = 0.0\n",
    "    total_loss = 0.0\n",
    "    loss_func = nn.BCELoss() if task_type else loss_RNNtask\n",
    "    with torch.no_grad():\n",
    "        for x, y_t in dataloader:\n",
    "            b_size = x.shape[0]\n",
    "            # Bemindful of shape changes\n",
    "            y_p = model(x, task_type = task_type)\n",
    "\n",
    "            if 0 == task_type:\n",
    "                '''\n",
    "                mask : [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "                y_p  : [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "                y_t  : [batch_size, n_pairs, 2*n_pairs]\n",
    "                '''\n",
    "                mask = model.get_mask(b_size)\n",
    "                y_p = y_p * mask \n",
    "                y_t = y_t * mask[:,:,:,0]\n",
    "            # Calculate Loss\n",
    "            loss = loss_func(y_p, y_t).clone().detach()\n",
    "            \n",
    "            if 0 != task_type:\n",
    "                # Calculate Acc only for FF task\n",
    "                preds = y_p > 0.5\n",
    "                acc = (preds == y_t).sum().item() / len(y_t)\n",
    "                total_acc += acc\n",
    "            \n",
    "            # Record statistics\n",
    "            total_loss += loss\n",
    "            \n",
    "    # Back to Train Mode\n",
    "    model.train()\n",
    "    return total_loss / len(dataloader), total_acc / len(dataloader)\n",
    "\n",
    "\n",
    "def train(model, train_dataloader, test_dataloader = None, n_epochs = 20, \n",
    "          task_type = 0, lr = 0.01):\n",
    "    '''\n",
    "    task_type = 0 for RNN 1 for FF\n",
    "    \n",
    "    '''\n",
    "    loss_lst = []\n",
    "    acc_lst = []\n",
    "    test_acc_lst = []\n",
    "\n",
    "    loss_func = nn.BCELoss() if task_type else loss_RNNtask\n",
    "    # Using Adam by default\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
    "    model.train()\n",
    "    for e in range(n_epochs + 1):\n",
    "        for x, y_t in train_dataloader:\n",
    "            optimizer.zero_grad() # Clear gradients\n",
    "            b_size = x.shape[0]\n",
    "            y_p = mymodel(x, task_type = task_type)\n",
    "            \n",
    "            if 0 == task_type:\n",
    "                '''\n",
    "                mask : [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "                y_p  : [batch_size, n_pairs, 2*n_pairs, 2*n_pairs]\n",
    "                y_t  : [batch_size, n_pairs, 2*n_pairs]\n",
    "                '''\n",
    "                mask = model.get_mask(b_size)\n",
    "                y_p = y_p * mask \n",
    "                y_t = y_t * mask[:,:,:,0]\n",
    "\n",
    "            loss = loss_func(y_p, y_t)\n",
    "            # Backward Pass\n",
    "            #loss.backward(retain_graph=True) ### Is the same graph accessed multiple times over different backward pass?\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Record Statistics\n",
    "            loss_lst.append(loss.clone().detach())\n",
    "            acc = 0.0\n",
    "            if 0 != task_type:\n",
    "                # Calculate Acc only for FF task\n",
    "                preds = y_p > 0.5\n",
    "                acc = (preds == y_t).sum().item() / len(y_t)\n",
    "                acc_lst.append(acc)\n",
    "\n",
    "        if 0 == (e) % 10:\n",
    "            t_loss = 0.0\n",
    "            t_acc = 0.0\n",
    "            if test_dataloader is not None:\n",
    "                t_loss, t_acc = test(model, test_dataloader, task_type, loss_func)\n",
    "            print(f'Epoch [{e}/{n_epochs}]:\\t--LastBatchLoss:{loss:.3f}, TrainAcc:{acc:.3f}, TestLoss:{t_loss:.3f}, TestAcc:{t_acc:.3f}')\n",
    "\n",
    "    return loss_lst, acc_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use (RNN Task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 1:\n",
      "inputs: [[0, 1], [0, 2], [2, 1], [0, 1], [0, 1]]\n",
      "outputs: [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 2, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 2, 2, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 2, 2, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 2, 2]]\n",
      "\n",
      "processed targets: [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 3, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 3, 3, 0, 0, 0, 0], [0, 0, 0, 1, 1, 1, 3, 3, 0, 0], [0, 0, 0, 0, 1, 1, 1, 1, 3, 3]]\n",
      "labels: tensor([0, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "def undo_one_hot(item_list):\n",
    "  try:\n",
    "    converted = []\n",
    "    for sub_list in item_list:\n",
    "      new_sub_list = []\n",
    "      for item in sub_list:\n",
    "        new_item = [key for key in items_dict.keys() if (torch.tensor(items_dict[key]) == item).all()]\n",
    "        if len(new_item):\n",
    "            new_sub_list.append(new_item[0])\n",
    "        else:\n",
    "            new_sub_list.append(0)\n",
    "        '''\n",
    "        for key in items_dict.keys():\n",
    "          if (torch.tensor(items_dict[key]) == item).all():\n",
    "              new_sub_list.append(key)\n",
    "          else:\n",
    "              new_sub_list.append(0)\n",
    "        '''\n",
    "      converted.append(new_sub_list)\n",
    "    return converted\n",
    "  except:\n",
    "    return item_list\n",
    "\n",
    "def to_indices(converted_in, converted_out):\n",
    "    '''\n",
    "    Convert the de-one-hot version of output further into sequences of input elements indices\n",
    "    '''\n",
    "    new_lst = []\n",
    "    for trial_in, trial_out in zip(converted_in, converted_out):\n",
    "        new_trial = []\n",
    "        flat_trial_in = torch.tensor(trial_in).flatten().tolist()\n",
    "        #print(flat_trial_in)\n",
    "        for i in range(len(trial_in)):\n",
    "            #print(flat_trial_in[: 2*(i+1)])\n",
    "            indices_lst = {}\n",
    "            for idx, n in enumerate(flat_trial_in[: 2*(i+1)]):\n",
    "                if n not in indices_lst.keys():\n",
    "                    indices_lst[n] = idx\n",
    "            indices_lst[0] = 0\n",
    "            #print(indices_lst)\n",
    "            #print('='*50)\n",
    "            new_trial.append([indices_lst[n] for n in trial_out[i]])\n",
    "        new_lst.append(new_trial)\n",
    "    return new_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trial 1:\n",
      "inputs: [[1, 0], [0, 1], [1, 2], [1, 2], [1, 2]]\n",
      "outputs: [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 0, 0, 0, 0, 0, 0], [0, 0, 1, 1, 1, 2, 0, 0, 0, 0], [0, 0, 1, 1, 1, 1, 2, 2, 0, 0], [0, 0, 1, 1, 1, 1, 1, 2, 2, 2]]\n",
      "\n",
      "processed targets: [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 5, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 5, 5, 0, 0], [0, 0, 0, 0, 0, 0, 0, 5, 5, 5]]\n",
      "labels: tensor([0, 1, 1, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Prepare Data for RNN tasks\n",
    "'''\n",
    "params_dict = {\n",
    "    'batch_size': 10, # Should be the same as n_pairs * n_trials\n",
    "    'n_epochs': 200,\n",
    "    'dataset_size': 1000,\n",
    "    'learning_rate': 0.0005,\n",
    "    'momentum': .99,\n",
    "    'task': 'si',\n",
    "    'n_trials': 1, \n",
    "    'input_length': 3,\n",
    "    'n_pairs':5, \n",
    "}\n",
    "\n",
    "# Prepare Data\n",
    "stimulus_dict = make_stimulus_dict(params_dict['input_length'])\n",
    "\n",
    "items_dict = {i : stimulus_dict[i] for i in range(len(stimulus_dict))}\n",
    "#items_rank_dict = {v: k for k, v in items_dict.items()}\n",
    "\n",
    "trials = get_RNN_batch(params_dict['n_trials'], params_dict['n_pairs'], stimulus_dict, params_dict['input_length'], pad=True)\n",
    "\n",
    "inputs_converted = []\n",
    "outputs_converted = []\n",
    "# Hard coded for now \n",
    "labels = torch.tensor([0, 1, 1, 0, 1])\n",
    "\n",
    "for i in range(len(trials)):\n",
    "  inputs_converted.append(undo_one_hot(trials[i][0]))\n",
    "  outputs_converted.append(undo_one_hot(trials[i][1]))\n",
    "\n",
    "processed_tgt = to_indices(inputs_converted, outputs_converted)\n",
    "\n",
    "for i in range(len(trials)):\n",
    "  print(f'trial {i+1}:')\n",
    "  print(f'inputs: {inputs_converted[i]}')\n",
    "  print(f'outputs: {outputs_converted[i]}\\n')\n",
    "  print(f'processed targets: {processed_tgt[i]}')\n",
    "  print(f'labels: {labels}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Input shape of the model must be [batch_size = num_trials, n_pairs, -1]\n",
    "# Prepare the generated data for train/test \n",
    "'''\n",
    "x = torch.tensor([t[0].reshape(params_dict['n_pairs'], 2*params_dict['input_length']).tolist() for t in trials])\n",
    "processed_data = [[x, torch.tensor(processed_tgt)]]\n",
    "# first trial in processed data\n",
    "test_x = processed_data[0][0] # Same as \n",
    "test_y = processed_data[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!!!Copied from above!!!\n",
    "params_dict = {\n",
    "    'batch_size': 10, # Should be the same as n_pairs * n_trials\n",
    "    'n_epochs': 200,\n",
    "    'dataset_size': 1000,\n",
    "    'learning_rate': 0.0005,\n",
    "    'momentum': .99,\n",
    "    'task': 'si',\n",
    "    'n_trials': 1, \n",
    "    'input_length': 3,\n",
    "    'n_pairs':5, \n",
    "    'hidden_dim': 4\n",
    "}\n",
    "'''\n",
    "trunk_net = MLP(params_dict['input_length'] * 2, params_dict['hidden_dim'], params_dict['hidden_dim'])\n",
    "\n",
    "mymodel = MyModel(trunk_net, params_dict['hidden_dim'], params_dict['n_pairs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 1., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[1., 0., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [0., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [0., 0., 0.],\n",
       "          [0., 0., 0.]],\n",
       "\n",
       "         [[0., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.],\n",
       "          [0., 0., 1.],\n",
       "          [0., 1., 0.],\n",
       "          [1., 0., 0.],\n",
       "          [0., 1., 0.]]]])"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Infering the order based on the input\n",
    "mymodel.infer_orders(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20]:\t--LastBatchLoss:2.211, TrainAcc:0.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [10/20]:\t--LastBatchLoss:2.177, TrainAcc:0.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [20/20]:\t--LastBatchLoss:2.154, TrainAcc:0.000, TestLoss:0.000, TestAcc:0.000\n"
     ]
    }
   ],
   "source": [
    "# Train on Task 0\n",
    "loss_lst, acc_lst = train(mymodel, processed_data, test_data_loader = None, \n",
    "                          n_epochs = 20, task_type = 0, lr = 0.05) # May need larger Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.2896), 0.0)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test Model\n",
    "test(mymodel, processed_data, task_type = 0, lr = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use (Transitive Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_dict = make_stimulus_dict(params_dict['input_length'])\n",
    "# Create 3 batches\n",
    "TI_data = [get_batch(params_dict['n_pairs'] * params_dict['n_trials'], params_dict['task'], stimulus_dict, params_dict['input_length']) for _ in range(3)]\n",
    "# Reshaping the input ** must be of shape [n_trials, n_pairs, 2 * feat_dim = trunk_net.input_dim]\n",
    "TI_data = [(sti.reshape(params_dict['n_trials'], params_dict['n_pairs'], -1), tgt) for sti, tgt in TI_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40]:\t--LastBatchLoss:0.618, TrainAcc:0.600, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [10/40]:\t--LastBatchLoss:0.639, TrainAcc:0.600, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [20/40]:\t--LastBatchLoss:0.535, TrainAcc:0.400, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [30/40]:\t--LastBatchLoss:0.067, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [40/40]:\t--LastBatchLoss:0.002, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n"
     ]
    }
   ],
   "source": [
    "loss_lst, acc_lst = train(mymodel, TI_data, n_epochs = 40, task_type = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Use (Subset Inclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "stimulus_dict = make_stimulus_dict(params_dict['input_length'])\n",
    "# Create 3 batches\n",
    "SI_data = [get_batch(params_dict['n_pairs'] * params_dict['n_trials'], 'si', stimulus_dict, params_dict['input_length']) \n",
    "           for _ in range(3)]\n",
    "# Reshaping the input ** must be of shape [n_trials, n_pairs, 2 * feat_dim = trunk_net.input_dim]\n",
    "SI_data = [(sti.reshape(params_dict['n_trials'], params_dict['n_pairs'], -1), tgt) for sti, tgt in SI_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/40]:\t--LastBatchLoss:0.152, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [10/40]:\t--LastBatchLoss:0.154, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [20/40]:\t--LastBatchLoss:0.182, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [30/40]:\t--LastBatchLoss:0.017, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n",
      "Epoch [40/40]:\t--LastBatchLoss:0.076, TrainAcc:1.000, TestLoss:0.000, TestAcc:0.000\n"
     ]
    }
   ],
   "source": [
    "loss_lst, acc_lst = train(mymodel, SI_data, n_epochs = 40, task_type = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10, 10])"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = mymodel(test_x)\n",
    "out.shape # [batch_size = n_trials, n_pairs, 2*n_pairs, 2*n_pairs] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 10, 10])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "k-th pair of input, matrix 2n_pairs x 2n_pairs (4 , 4)\n",
    "2 = n_pairs\n",
    "[a,b] -> [a, b, -, -] \n",
    "[c,b] -> [a, b, b, c]\n",
    "\n",
    "index \n",
    "0 a\n",
    "1 b\n",
    "2 c\n",
    "3 b\n",
    "\n",
    "matrix 0 for input pair 0\n",
    "[0.6, 0.4, 0, 0]    [0] argmax a\n",
    "[0.2, 0.8, 0, 0]    [1]        b\n",
    "[0, 0, 0, 0]        []\n",
    "[0, 0, 0, 0]        []\n",
    "sequence of order is of len 4\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 4])"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk_net(test_x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "----\n",
    "### Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Task design\n",
    "def int2bits(n, width):\n",
    "    '''\n",
    "    Convert an Integer to Its Binary Representation as a List/Array/Tensor\n",
    "    e.g. n = 3, width = 4 -> [0, 0, 1, 1]\n",
    "    '''\n",
    "    return [int(b) for b in bin(n)[2:].zfill(width)] # bin() returns a str with prefix '0b'\n",
    "    \n",
    "\n",
    "\n",
    "def gen_batch(batch_size, n_elements = 3, \n",
    "              task = 'ti'):\n",
    "    '''\n",
    "    Generate a Batch of Stimuli (list) for the Desired Task\n",
    "    \n",
    "    Inputs:\n",
    "    - batch_size   : (int) num of stimulus\n",
    "    - n_elements   : (int) Total number of elements on the base set\n",
    "    - task         : (str)\n",
    "                     'ti' - Transitive Inference; without Reflexitivity (relation with itself)\n",
    "                     'si' - Subset Inclusion; \n",
    "                     'div'- Divisibility; without 0\n",
    "\n",
    "    Returns: [[A, B], label] x batch_size       ## of shape [batch_size, 2, n_elements]\n",
    "    '''\n",
    "    assert task in ['ti',\n",
    "                    'si',\n",
    "                    'div'], f'Requested Task [{task}] Not Supported!'\n",
    "    stimuli = []\n",
    "    if 'ti' == task.lower() or 'div' == task.lower():\n",
    "        max_b_size = int(n_elements)\n",
    "        # Transitive Inference\n",
    "        if 'ti' == task.lower():\n",
    "            # All possible instances\n",
    "            stimulus_dict = np.eye(n_elements, dtype = int)\n",
    "            np.random.shuffle(stimulus_dict)\n",
    "            # Randomly sample two instances in the dict as a stimulus\n",
    "            for pair_id in range(batch_size):\n",
    "                dict_indices = random.sample(range(max_b_size), k = 2) # without replacement\n",
    "                # target = which instance is greater\n",
    "                target = 0 if dict_indices[0] > dict_indices[1] else 1\n",
    "                stimuli.append([stimulus_dict[dict_indices].tolist(), target])\n",
    "        else: # Divisibility\n",
    "            for pair_id in range(batch_size):\n",
    "                stimulus_dict = random.sample(range(1, max_b_size + 1), k = 2) # excluding 0\n",
    "                # target = if the previous is divisible by the latter\n",
    "                target = int(0 == stimulus_dict[0] % stimulus_dict[1])\n",
    "                stimuli.append([stimulus_dict, target])\n",
    "    # Subset Inclusion\n",
    "    elif 'si' == task.lower():\n",
    "        max_b_size = 2 ** int(n_elements)\n",
    "        # randomly sample two instances\n",
    "        for pair_id in range(batch_size):\n",
    "            idx_A, idx_B = random.sample(range(max_b_size), k = 2)\n",
    "            # target if the previous is a superset of the latter\n",
    "            A, B = np.array(int2bits(idx_A, width = n_elements)), np.array(int2bits(idx_B, n_elements))\n",
    "            element_indices_B = np.arange(n_elements)[1 == B]\n",
    "            target = int(A[element_indices_B].all()) # if non-zero entries in B are also in A\n",
    "            stimuli.append([[A.tolist(), B.tolist()], target])\n",
    "    return stimuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def get_test_data(n):\n",
    "    \"\"\"Generate test data for relational tasks.\n",
    "    \"\"\"\n",
    "    test_x = np.zeros((n, n, 2*n))\n",
    "    for i, j in itertools.product(range(n), range(n)):\n",
    "        test_x[i,j,i] = 1 \n",
    "        test_x[i,j,n+j] = 1 \n",
    "    test_x = torch.from_numpy(test_x).float()\n",
    "    test_x = test_x/torch.sqrt(torch.tensor(2))\n",
    "    return test_x\n",
    "\n",
    "def get_transitive_data(n):\n",
    "    \"\"\"Generate training data for TI task.\n",
    "    \"\"\"\n",
    "    test_x = get_test_data(n)\n",
    "    x = test_x[tuple(zip(*([(i, i+1) for i in range(n-1)] + [(i+1, i) for i in range(n-1)])))]\n",
    "    y = torch.tensor([1.]*(n-1)+[-1.]*(n-1))\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_transitive_data(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.7071],\n",
       "        [0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000,\n",
       "         0.0000],\n",
       "        [0.0000, 0.0000, 0.0000, 0.0000, 0.7071, 0.0000, 0.0000, 0.0000, 0.7071,\n",
       "         0.0000]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
